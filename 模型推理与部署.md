本项目中的模型主要支持以下推理和部署方式：

### llama.cpp
提供了一种模型量化和在本地CPU、GPU上部署方式。

教程：[llama.cpp量化部署](./llama.cpp量化部署)

### 🤗Transformers
提供原生transformers推理接口，支持CPU/GPU上进行模型推理。

教程：[使用Transformers推理](./使用Transformers推理)

### text-generation-webui
提供了一种可实现前端UI界面的部署方式。

教程：[使用text-generation-webui搭建界面](./使用text-generation-webui搭建界面)

### LlamaChat
提供了一种基于macOS系统的图形交互界面，支持GGML（`.bin`格式）和PyTorch（`.pth`格式）版本的模型加载。

教程：[使用LlamaChat图形界面（macOS）](./使用LlamaChat图形界面（macOS）)

### LangChain
[LangChain](https://github.com/hwchase17/langchain)是一个用于开发由LLM驱动的应用程序的框架，旨在帮助开发人员使用LLM构建端到端的应用程序。
借助LangChain提供的组件和接口，开发人员可以方便地设计与搭建诸如问答、摘要、聊天机器人、代码理解、信息提取等多种基于LLM能力的应用程序。

教程：[与LangChain进行集成](./与LangChain进行集成)

### privateGPT
[privateGPT](https://github.com/imartinez/privateGPT) 是基于[llama-cpp-python](https://github.com/abetlen/llama-cpp-python)和[LangChain](https://github.com/hwchase17/langchain)等的一个开源项目，旨在提供本地化文档分析并利用大模型来进行交互问答的接口。用户可以利用privateGPT对本地文档进行分析，并且利用GPT4All或llama.cpp兼容的大模型文件对文档内容进行提问和回答，确保了数据本地化和私有化。

教程：[使用privateGPT进行多文档问答](./使用privateGPT进行多文档问答)