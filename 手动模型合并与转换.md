### å‡†å¤‡å·¥ä½œ

1. ç¡®ä¿æœºå™¨æœ‰è¶³å¤Ÿçš„å†…å­˜åŠ è½½å®Œæ•´æ¨¡å‹ï¼ˆä¾‹å¦‚7Bæ¨¡å‹éœ€è¦13-15Gï¼‰ä»¥è¿›è¡Œåˆå¹¶æ¨¡å‹æ“ä½œã€‚
2. åŠ¡å¿…ç¡®è®¤åŸºæ¨¡å‹å’Œä¸‹è½½çš„LoRAæ¨¡å‹å®Œæ•´æ€§ï¼Œæ£€æŸ¥æ˜¯å¦ä¸[SHA256.md](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md)æ‰€ç¤ºçš„å€¼ä¸€è‡´ï¼Œå¦åˆ™æ— æ³•è¿›è¡Œåˆå¹¶æ“ä½œã€‚åŸç‰ˆLLaMAåŒ…å«ï¼š`tokenizer.model`ã€`tokenizer_checklist.chk`ã€`consolidated.*.pth`ã€`params.json`
3. ä¸»è¦ä¾èµ–åº“å¦‚ä¸‹ï¼ˆå¦‚æœå‡ºé—®é¢˜å°±è¯·å®‰è£…ä»¥ä¸‹æŒ‡å®šç‰ˆæœ¬ï¼‰ï¼š
   - `torch`ï¼ˆ1.10,1.12,1.13æµ‹è¯•é€šè¿‡)
   - `transformers`ï¼ˆ4.28.1æµ‹è¯•é€šè¿‡ï¼‰
   - `sentencepiece`ï¼ˆ0.1.97æµ‹è¯•é€šè¿‡ï¼‰
   - `peft`ï¼ˆ0.3.0devæµ‹è¯•é€šè¿‡ï¼‰
   - pythonç‰ˆæœ¬å»ºè®®åœ¨3.9ä»¥ä¸Š

```bash
pip install torch==1.13.1
pip install transformers
pip install sentencepiece
pip install git+https://github.com/huggingface/peft
```

*æ³¨æ„ï¼šæœ¬é¡¹ç›®ä¸å¯¹ä½¿ç”¨ç¬¬ä¸‰æ–¹ï¼ˆéFacebookå®˜æ–¹ï¼‰æƒé‡çš„åˆè§„æ€§å’Œæ­£ç¡®æ€§è´Ÿè´£ï¼Œä¾‹å¦‚HuggingFaceæ¨¡å‹åº“ä¸­çš„`decapoda-research/llama-7b-hf`ï¼ˆuse at your own riskï¼‰ã€‚*


### Step 1: å°†åŸç‰ˆLLaMAæ¨¡å‹è½¬æ¢ä¸ºHFæ ¼å¼

è¯·ä½¿ç”¨[ğŸ¤—transformers](https://huggingface.co/docs/transformers/installation#install-from-source)æä¾›çš„è„šæœ¬[convert_llama_weights_to_hf.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)ï¼Œå°†åŸç‰ˆLLaMAæ¨¡å‹è½¬æ¢ä¸ºHuggingFaceæ ¼å¼ã€‚å°†åŸç‰ˆLLaMAçš„`tokenizer.model`æ”¾åœ¨`--input_dir`æŒ‡å®šçš„ç›®å½•ï¼Œå…¶ä½™æ–‡ä»¶æ”¾åœ¨`${input_dir}/${model_size}`ä¸‹ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åï¼Œ`--output_dir`ä¸­å°†å­˜æ”¾è½¬æ¢å¥½çš„HFç‰ˆæƒé‡ã€‚

```bash
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir path_to_original_llama_root_dir \
    --model_size 7B \
    --output_dir path_to_original_llama_hf_dir
```

### Step 2: åˆå¹¶LoRAæƒé‡ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡

è¿™ä¸€æ­¥éª¤ä¼šå¯¹åŸç‰ˆLLaMAæ¨¡å‹ï¼ˆHFæ ¼å¼ï¼‰æ‰©å……ä¸­æ–‡è¯è¡¨ï¼Œåˆå¹¶LoRAæƒé‡å¹¶ç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚æ­¤å¤„å¯ä»¥é€‰æ‹©è¾“å‡ºPyTorchç‰ˆæœ¬æƒé‡ï¼ˆ`.pth`æ–‡ä»¶ï¼‰æˆ–è€…è¾“å‡ºHuggingFaceç‰ˆæœ¬æƒé‡ï¼ˆ`.bin`æ–‡ä»¶ï¼‰ã€‚

`.pth`æ–‡ä»¶å¯ç”¨äºï¼š

  - [ä½¿ç”¨llama.cppå·¥å…·è¿›è¡Œé‡åŒ–å’Œéƒ¨ç½²](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cppé‡åŒ–éƒ¨ç½²)

`.bin`æ–‡ä»¶å¯ç”¨äºï¼š

  - [ä½¿ç”¨Transformersè¿›è¡Œæ¨ç†](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨Transformersæ¨ç†)
  - [ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢)

æ³¨æ„ï¼Œä¸åŒæ¨¡å‹çš„åˆå¹¶æ–¹å¼ä¸åŒã€‚è¯·é˜…è¯»ä»¥ä¸‹æŒ‡å—å¹¶ä¸¥æ ¼æŒ‰ç…§æ­¥éª¤è¿›è¡Œã€‚

#### å•LoRAæƒé‡åˆå¹¶ï¼ˆé€‚ç”¨äº Chinese-LLaMA, Chinese-LLaMA-Plus, Chinese-Alpacaï¼‰

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_or_alpaca_lora \
    --output_type [pth|huggingface] \
    --output_dir path_to_output_dir 
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼ˆStep 1ç”Ÿæˆï¼‰
- `--lora_model`ï¼šä¸­æ–‡LLaMA/Alpaca LoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨[ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main#model-hub)
- `--output_type`: æŒ‡å®šè¾“å‡ºæ ¼å¼ï¼Œå¯ä¸º`pth`æˆ–`huggingface`ã€‚è‹¥ä¸æŒ‡å®šï¼Œé»˜è®¤ä¸º`pth`
- `--output_dir`ï¼šæŒ‡å®šä¿å­˜å…¨é‡æ¨¡å‹æƒé‡çš„ç›®å½•ï¼Œé»˜è®¤ä¸º`./`
- ï¼ˆå¯é€‰ï¼‰`--offload_dir`ï¼šå¯¹äºä½å†…å­˜ç”¨æˆ·éœ€è¦æŒ‡å®šä¸€ä¸ªoffloadç¼“å­˜è·¯å¾„

#### å¤šLoRAæƒé‡åˆå¹¶ï¼ˆé€‚ç”¨äºChinese-Alpaca-Plusï¼‰

åˆå¹¶Chinese-Alpaca-Pluséœ€è¦æä¾›ä¸¤ä¸ªLoRAæƒé‡ï¼Œåˆ†åˆ«ä¸ºChinese-LLaMA-Plus-LoRAå’ŒChinese-Alpaca-Plus-LoRAã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®Œæˆåˆå¹¶ï¼š

```bash
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_plus_lora,path_to_chinese_alpaca_plus_lora \
    --output_type [pth|huggingface] \
    --output_dir path_to_output_dir 
```

å‚æ•°é€‰é¡¹å«ä¹‰ä¸å•LoRAæƒé‡åˆå¹¶ä¸­çš„å«ä¹‰ç›¸åŒã€‚
éœ€è¦æ³¨æ„çš„æ˜¯` --lora_model`å‚æ•°åè¦æä¾›ä¸¤ä¸ªlora_modelçš„åœ°å€ï¼Œç”¨é€—å·åˆ†éš”ã€‚

âš ï¸ **ä¸¤ä¸ªLoRAæ¨¡å‹çš„é¡ºåºå¾ˆé‡è¦ï¼Œä¸èƒ½é¢ å€’ã€‚å…ˆå†™LLaMA-Plus-LoRAç„¶åå†™Alpaca-Plus-LoRAã€‚** âš ï¸

