### 准备工作

1. 运行前确保拉取仓库最新版代码：`git pull`
2. 确保机器有足够的内存加载完整模型（例如7B模型需要13-15G）以进行合并模型操作。
3. **务必确认基模型和下载的LoRA模型完整性，检查是否与[SHA256.md](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md)所示的值一致**，否则无法进行合并操作。原版LLaMA包含：`tokenizer.model`、`tokenizer_checklist.chk`、`consolidated.*.pth`、`params.json`
4. 主要依赖库如下（python>=3.9），**请安装指定版本，否则合并后无法比对SHA256校验值**：

```bash
pip install torch==1.13.1
pip install transformers==4.28.1
pip install sentencepiece==0.1.97
pip install peft==0.3.0
```

*注意：经过多方比对，HuggingFace模型库中的`elinas/llama-7b-hf-transformers-4.29`与原版llama模型离线转换为HF格式后的SHA256一致（已验证7B/13B/33B）。如果你要使用，则应确保满足相应使用许可，我们不对其合规性做出任何保证（use at your own risk）。*


### Step 1: 将原版LLaMA模型转换为HF格式

请使用[🤗transformers](https://huggingface.co/docs/transformers/installation#install-from-source)提供的脚本[convert_llama_weights_to_hf.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)，将原版LLaMA模型转换为HuggingFace格式。将原版LLaMA的`tokenizer.model`放在`--input_dir`指定的目录，其余文件放在`${input_dir}/${model_size}`下。执行以下脚本（注意这个脚本路径指的是transformers的路径，不是本项目的目录）后，`--output_dir`中将存放转换好的HF版权重。

```bash
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir path_to_original_llama_root_dir \
    --model_size 7B \
    --output_dir path_to_original_llama_hf_dir
```

`--output_dir`目录下会生成HF格式的模型文件，诸如：

```
config.json
generation_config.json
pytorch_model-00001-of-00002.bin
pytorch_model-00002-of-00002.bin
pytorch_model.bin.index.json
special_tokens_map.json
tokenizer_config.json
tokenizer.json
tokenizer.model
```

### Step 2: 合并LoRA权重，生成全量模型权重

**[New]** **请优先使用新版合并脚本，所需内存显著降低，只需将以下命令中的脚本替换为`scripts/merge_llama_with_chinese_lora_low_mem.py`。注意：已知使用low_mem脚本在macOS下转换33B模型可能会出现一定误差，请避免以该组合合并。**

这一步骤会对原版LLaMA模型（HF格式）扩充中文词表，合并LoRA权重并生成全量模型权重。此处可以选择输出PyTorch版本权重（`.pth`文件）或者输出HuggingFace版本权重（`.bin`文件）。**请优先转为pth文件，比对合并后模型的SHA256无误后按需再转成HF格式。**

- `.pth`文件可用于：[使用llama.cpp工具进行量化和部署](./llama.cpp量化部署) 

- `.bin`文件可用于：[使用Transformers进行推理](./使用Transformers推理)、[使用text-generation-webui搭建界面](./使用text-generation-webui搭建界面)

注意，不同模型的合并方式不同。请阅读以下指南并严格按照步骤进行。

#### 单LoRA权重合并（适用于 Chinese-LLaMA, Chinese-LLaMA-Plus, Chinese-Alpaca）

执行以下命令：

```bash
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_or_alpaca_lora \
    --output_type [pth|huggingface] \
    --output_dir path_to_output_dir 
```

参数说明：

- `--base_model`：存放HF格式的LLaMA模型权重和配置文件的目录（Step 1生成）
- `--lora_model`：中文LLaMA/Alpaca LoRA解压后文件所在目录，也可使用[🤗Model Hub模型调用名称](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main#model-hub)
- `--output_type`: 指定输出格式，可为`pth`或`huggingface`。若不指定，默认为`pth`
- `--output_dir`：指定保存全量模型权重的目录，默认为`./`
- （可选）`--offload_dir`：对于低内存用户需要指定一个offload缓存路径

#### 多LoRA权重合并（适用于Chinese-Alpaca-Plus）

合并Chinese-Alpaca-Plus需要提供两个LoRA权重，分别为Chinese-LLaMA-Plus-LoRA和Chinese-Alpaca-Plus-LoRA。执行以下命令完成合并：

```bash
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_plus_lora,path_to_chinese_alpaca_plus_lora \
    --output_type [pth|huggingface] \
    --output_dir path_to_output_dir 
```

参数选项含义与单LoRA权重合并中的含义相同。需要注意的是` --lora_model`参数后要提供两个lora_model的地址，用逗号分隔。⚠️ **两个LoRA模型的顺序很重要，不能颠倒。先写LLaMA-Plus-LoRA然后写Alpaca-Plus-LoRA。** 

### Step 3: 合并后检查（重要！）

**合并完成后务必检查SHA256！合并完成后务必检查SHA256！合并完成后务必检查SHA256！**

- 合并后pth文件的SHA256：https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md
- 推荐先转成pth格式，比对SHA256无误后，如有需要再转成HF格式，因为HF格式对应的模型SHA256经常发生变化（meta信息改变）
- 另外，也可参考我们的[解码示例](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples)，使用相同的解码参数进行测试。如果多次运行后结果与示例相差较大，则可能表示合并后的模型可能存在不完整等问题。**强烈建议比对以上SHA256值，确保模型正确。**
