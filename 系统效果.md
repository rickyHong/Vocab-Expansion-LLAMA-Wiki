为了快速评测相关模型的实际文本生成表现，本项目在给定相同的prompt的情况下，在一些常见任务上对比测试了本项目的中文Alpaca-7B、中文Alpaca-13B、中文Alpaca-33B、中文Alpaca-Plus-7B、中文Alpaca-Plus-13B的效果。生成回复具有随机性，受解码超参、随机种子等因素影响。以下相关评测并非绝对严谨，测试结果仅供晾晒参考，欢迎自行体验。详细评测结果请查看[examples目录](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples)。

| 测试任务         | 样例数 | Alpaca-Plus-7B | Alpaca-Plus-13B | Alpaca-33B |
| ---------------- | :----: | :------------: | :-------------: | :--------: |
| **💯总平均分**    |  200   |      75.3      |      79.4       | 👍🏻**82.0** |
| 知识问答         |   20   |      70.5      |      79.5       | 👍🏻**82.3** |
| 开放式问答       |   20   |   👍🏻**80.5**   |    👍🏻**80**     |    78.5    |
| 数值计算、推理   |   20   |       51       |      61.5       | 👍🏻**84.5** |
| 诗词、文学、哲学 |   20   |      78.5      |   **👍🏻81.3**    |     76     |
| 音乐、体育、娱乐 |   20   |      72.3      |   👍🏻**76.8**    |    72.5    |
| 写信、写文章     |   20   |       81       |   👍🏻**86.5**    |     79     |
| 文本翻译         |   20   |      86.8      |      89.3       | 👍🏻**92.3** |
| 多轮交互         |   20   |      80.3      |   👍🏻**81.3**    |     78     |
| 代码编程         |   20   |      62.5      |      67.5       | 👍🏻**84.0** |
| 伦理、拒答       |   20   |      89.8      |      90.5       | 👍🏻**92.5** |