为了快速评测相关模型的实际表现，本项目在给定相同的prompt的情况下，在一些常见任务上对比测试了本项目的中文Alpaca-7B、中文Alpaca-13B、中文Alpaca-Plus-7B、中文Alpaca-Plus-13B的效果。生成回复具有随机性，受解码超参、随机种子等因素影响。以下相关评测并非绝对严谨，测试结果仅供晾晒参考，欢迎自行体验。详细评测结果请查看[examples目录](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples)。

| 测试任务         | 样例数 | Alpaca-13B | Alpaca-Plus-7B | Alpaca-Plus-13B |
| ---------------- | :----: | :--------: | :------------: | :-------------: |
| **💯总平均分**    |  200   |    74.3    |      78.2      |   **👍🏻80.8**    |
| 知识问答         |   20   |     70     |       74       |    **👍🏻79**     |
| 开放式问答       |   20   |     77     |       77       |       77        |
| 数值计算、推理   |   20   |     61     |       61       |       60        |
| 诗词、文学、哲学 |   20   |     65     |    **👍🏻76**    |    **👍🏻76**     |
| 音乐、体育、娱乐 |   20   |     68     |       73       |    **👍🏻80**     |
| 写信、写文章     |   20   |     83     |       82       |    **👍🏻87**     |
| 文本翻译         |   20   |     84     |       87       |    **👍🏻90**     |
| 多轮交互         |   20   |     88     |       89       |       89        |
| 代码编程         |   20   |     65     |       64       |    **👍🏻70**     |
| 伦理、拒答       |   20   |     82     |    **👍🏻99**    |    **👍🏻100**    |